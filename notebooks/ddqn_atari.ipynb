{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of double deep-Q learning initially taken from https://github.com/fg91/Deep-Q-Learning/blob/master/DQN.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import gym\n",
    "import tensorflow as tf\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrameProcessor:\n",
    "    \"\"\"Resizes and converts RGB Atari frames to grayscale\"\"\"\n",
    "    def __init__(self, frame_height=84, frame_width=84):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            frame_height: Integer, Height of a frame of an Atari game\n",
    "            frame_width: Integer, Width of a frame of an Atari game\n",
    "        \"\"\"\n",
    "        self.frame_height = frame_height\n",
    "        self.frame_width = frame_width\n",
    "        self.frame = tf.placeholder(shape=[210, 160, 3], dtype=tf.uint8)\n",
    "        self.processed = tf.image.rgb_to_grayscale(self.frame)\n",
    "        self.processed = tf.image.crop_to_bounding_box(self.processed, 34, 0, 160, 160)\n",
    "        self.processed = tf.image.resize_images(self.processed, \n",
    "                                                [self.frame_height, self.frame_width], \n",
    "                                                method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    \n",
    "    def process(self, session, frame):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            session: A Tensorflow session object\n",
    "            frame: A (210, 160, 3) frame of an Atari game in RGB\n",
    "        Returns:\n",
    "            A processed (84, 84, 1) frame in grayscale\n",
    "        \"\"\"\n",
    "        return session.run(self.processed, feed_dict={self.frame:frame})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    \"\"\"Implements a Deep Q Network\"\"\"\n",
    "    \n",
    "    # pylint: disable=too-many-instance-attributes\n",
    "    \n",
    "    def __init__(self, n_actions, hidden=1024, learning_rate=0.00001, \n",
    "                 frame_height=84, frame_width=84, agent_history_length=4):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n_actions: Integer, number of possible actions\n",
    "            hidden: Integer, Number of filters in the final convolutional layer. \n",
    "                    This is different from the DeepMind implementation\n",
    "            learning_rate: Float, Learning rate for the Adam optimizer\n",
    "            frame_height: Integer, Height of a frame of an Atari game\n",
    "            frame_width: Integer, Width of a frame of an Atari game\n",
    "            agent_history_length: Integer, Number of frames stacked together to create a state\n",
    "        \"\"\"\n",
    "        self.n_actions = n_actions\n",
    "        self.hidden = hidden\n",
    "        self.learning_rate = learning_rate\n",
    "        self.frame_height = frame_height\n",
    "        self.frame_width = frame_width\n",
    "        self.agent_history_length = agent_history_length\n",
    "        \n",
    "        self.input = tf.placeholder(shape=[None, self.frame_height, \n",
    "                                           self.frame_width, self.agent_history_length], \n",
    "                                    dtype=tf.float32)\n",
    "        # Normalizing the input\n",
    "        self.inputscaled = self.input/255\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv1 = tf.layers.conv2d(\n",
    "            inputs=self.inputscaled, filters=32, kernel_size=[8, 8], strides=4,\n",
    "            kernel_initializer=tf.variance_scaling_initializer(scale=2),\n",
    "            padding=\"valid\", activation=tf.nn.relu, use_bias=False, name='conv1')\n",
    "        self.conv2 = tf.layers.conv2d(\n",
    "            inputs=self.conv1, filters=64, kernel_size=[4, 4], strides=2, \n",
    "            kernel_initializer=tf.variance_scaling_initializer(scale=2),\n",
    "            padding=\"valid\", activation=tf.nn.relu, use_bias=False, name='conv2')\n",
    "        self.conv3 = tf.layers.conv2d(\n",
    "            inputs=self.conv2, filters=64, kernel_size=[3, 3], strides=1, \n",
    "            kernel_initializer=tf.variance_scaling_initializer(scale=2),\n",
    "            padding=\"valid\", activation=tf.nn.relu, use_bias=False, name='conv3')\n",
    "        self.conv4 = tf.layers.conv2d(\n",
    "            inputs=self.conv3, filters=hidden, kernel_size=[7, 7], strides=1, \n",
    "            kernel_initializer=tf.variance_scaling_initializer(scale=2),\n",
    "            padding=\"valid\", activation=tf.nn.relu, use_bias=False, name='conv4')\n",
    "        \n",
    "        # Splitting into value and advantage stream\n",
    "        self.valuestream, self.advantagestream = tf.split(self.conv4, 2, 3)\n",
    "        self.valuestream = tf.layers.flatten(self.valuestream)\n",
    "        self.advantagestream = tf.layers.flatten(self.advantagestream)\n",
    "        self.advantage = tf.layers.dense(\n",
    "            inputs=self.advantagestream, units=self.n_actions,\n",
    "            kernel_initializer=tf.variance_scaling_initializer(scale=2), name=\"advantage\")\n",
    "        self.value = tf.layers.dense(\n",
    "            inputs=self.valuestream, units=1, \n",
    "            kernel_initializer=tf.variance_scaling_initializer(scale=2), name='value')\n",
    "        \n",
    "        # Combining value and advantage into Q-values as described above\n",
    "        self.q_values = self.value + tf.subtract(self.advantage, tf.reduce_mean(self.advantage, axis=1, keepdims=True))\n",
    "        self.best_action = tf.argmax(self.q_values, 1)\n",
    "        \n",
    "        # The next lines perform the parameter update. This will be explained in detail later.\n",
    "        \n",
    "        # targetQ according to Bellman equation: \n",
    "        # Q = r + gamma*max Q', calculated in the function learn()\n",
    "        self.target_q = tf.placeholder(shape=[None], dtype=tf.float32)\n",
    "        # Action that was performed\n",
    "        self.action = tf.placeholder(shape=[None], dtype=tf.int32)\n",
    "        # Q value of the action that was performed\n",
    "        self.Q = tf.reduce_sum(tf.multiply(self.q_values, tf.one_hot(self.action, self.n_actions, dtype=tf.float32)), axis=1)\n",
    "        \n",
    "        # Parameter updates\n",
    "        self.loss = tf.reduce_mean(tf.losses.huber_loss(labels=self.target_q, predictions=self.Q))\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
    "        self.update = self.optimizer.minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionGetter:\n",
    "    \"\"\"Determines an action according to an epsilon greedy strategy with annealing epsilon\"\"\"\n",
    "    def __init__(self, n_actions, eps_initial=1, eps_final=0.1, eps_final_frame=0.01, \n",
    "                 eps_evaluation=0.0, eps_annealing_frames=1000000, \n",
    "                 replay_memory_start_size=50000, max_frames=25000000):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n_actions: Integer, number of possible actions\n",
    "            eps_initial: Float, Exploration probability for the first \n",
    "                replay_memory_start_size frames\n",
    "            eps_final: Float, Exploration probability after \n",
    "                replay_memory_start_size + eps_annealing_frames frames\n",
    "            eps_final_frame: Float, Exploration probability after max_frames frames\n",
    "            eps_evaluation: Float, Exploration probability during evaluation\n",
    "            eps_annealing_frames: Int, Number of frames over which the \n",
    "                exploration probabilty is annealed from eps_initial to eps_final\n",
    "            replay_memory_start_size: Integer, Number of frames during \n",
    "                which the agent only explores\n",
    "            max_frames: Integer, Total number of frames shown to the agent\n",
    "        \"\"\"\n",
    "        self.n_actions = n_actions\n",
    "        self.eps_initial = eps_initial\n",
    "        self.eps_final = eps_final\n",
    "        self.eps_final_frame = eps_final_frame\n",
    "        self.eps_evaluation = eps_evaluation\n",
    "        self.eps_annealing_frames = eps_annealing_frames\n",
    "        self.replay_memory_start_size = replay_memory_start_size\n",
    "        self.max_frames = max_frames\n",
    "        \n",
    "        # Slopes and intercepts for exploration decrease\n",
    "        self.slope = -(self.eps_initial - self.eps_final)/self.eps_annealing_frames\n",
    "        self.intercept = self.eps_initial - self.slope*self.replay_memory_start_size\n",
    "        self.slope_2 = -(self.eps_final - self.eps_final_frame)/(self.max_frames - self.eps_annealing_frames - self.replay_memory_start_size)\n",
    "        self.intercept_2 = self.eps_final_frame - self.slope_2*self.max_frames\n",
    "        \n",
    "    def get_action(self, session, frame_number, state, main_dqn, evaluation=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            session: A tensorflow session object\n",
    "            frame_number: Integer, number of the current frame\n",
    "            state: A (84, 84, 4) sequence of frames of an Atari game in grayscale\n",
    "            main_dqn: A DQN object\n",
    "            evaluation: A boolean saying whether the agent is being evaluated\n",
    "        Returns:\n",
    "            An integer between 0 and n_actions - 1 determining the action the agent perfoms next\n",
    "        \"\"\"\n",
    "        if evaluation:\n",
    "            eps = self.eps_evaluation\n",
    "        elif frame_number < self.replay_memory_start_size:\n",
    "            eps = self.eps_initial\n",
    "        elif frame_number >= self.replay_memory_start_size and frame_number < self.replay_memory_start_size + self.eps_annealing_frames:\n",
    "            eps = self.slope*frame_number + self.intercept\n",
    "        elif frame_number >= self.replay_memory_start_size + self.eps_annealing_frames:\n",
    "            eps = self.slope_2*frame_number + self.intercept_2\n",
    "        \n",
    "        if np.random.rand(1) < eps:\n",
    "            return np.random.randint(0, self.n_actions)\n",
    "        return session.run(main_dqn.best_action, feed_dict={main_dqn.input:[state]})[0]       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    \"\"\"Replay Memory that stores the last size=1,000,000 transitions\"\"\"\n",
    "    def __init__(self, size=1000000, frame_height=84, frame_width=84, \n",
    "                 agent_history_length=4, batch_size=32):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            size: Integer, Number of stored transitions\n",
    "            frame_height: Integer, Height of a frame of an Atari game\n",
    "            frame_width: Integer, Width of a frame of an Atari game\n",
    "            agent_history_length: Integer, Number of frames stacked together to create a state\n",
    "            batch_size: Integer, Number if transitions returned in a minibatch\n",
    "        \"\"\"\n",
    "        self.size = size\n",
    "        self.frame_height = frame_height\n",
    "        self.frame_width = frame_width\n",
    "        self.agent_history_length = agent_history_length\n",
    "        self.batch_size = batch_size\n",
    "        self.count = 0\n",
    "        self.current = 0\n",
    "        \n",
    "        # Pre-allocate memory\n",
    "        self.actions = np.empty(self.size, dtype=np.int32)\n",
    "        self.rewards = np.empty(self.size, dtype=np.float32)\n",
    "        self.frames = np.empty((self.size, self.frame_height, self.frame_width), dtype=np.uint8)\n",
    "        self.terminal_flags = np.empty(self.size, dtype=np.bool)\n",
    "        \n",
    "        # Pre-allocate memory for the states and new_states in a minibatch\n",
    "        self.states = np.empty((self.batch_size, self.agent_history_length, \n",
    "                                self.frame_height, self.frame_width), dtype=np.uint8)\n",
    "        self.new_states = np.empty((self.batch_size, self.agent_history_length, \n",
    "                                    self.frame_height, self.frame_width), dtype=np.uint8)\n",
    "        self.indices = np.empty(self.batch_size, dtype=np.int32)\n",
    "        \n",
    "    def add_experience(self, action, frame, reward, terminal):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            action: An integer between 0 and env.action_space.n - 1 \n",
    "                determining the action the agent perfomed\n",
    "            frame: A (84, 84, 1) frame of an Atari game in grayscale\n",
    "            reward: A float determining the reward the agend received for performing an action\n",
    "            terminal: A bool stating whether the episode terminated\n",
    "        \"\"\"\n",
    "        if frame.shape != (self.frame_height, self.frame_width):\n",
    "            raise ValueError('Dimension of frame is wrong!')\n",
    "        self.actions[self.current] = action\n",
    "        self.frames[self.current, ...] = frame\n",
    "        self.rewards[self.current] = reward\n",
    "        self.terminal_flags[self.current] = terminal\n",
    "        self.count = max(self.count, self.current+1)\n",
    "        self.current = (self.current + 1) % self.size\n",
    "             \n",
    "    def _get_state(self, index):\n",
    "        if self.count is 0:\n",
    "            raise ValueError(\"The replay memory is empty!\")\n",
    "        if index < self.agent_history_length - 1:\n",
    "            raise ValueError(\"Index must be min 3\")\n",
    "        return self.frames[index-self.agent_history_length+1:index+1, ...]\n",
    "        \n",
    "    def _get_valid_indices(self):\n",
    "        for i in range(self.batch_size):\n",
    "            while True:\n",
    "                index = random.randint(self.agent_history_length, self.count - 1)\n",
    "                if index < self.agent_history_length:\n",
    "                    continue\n",
    "                if index >= self.current and index - self.agent_history_length <= self.current:\n",
    "                    continue\n",
    "                if self.terminal_flags[index - self.agent_history_length:index].any():\n",
    "                    continue\n",
    "                break\n",
    "            self.indices[i] = index\n",
    "\n",
    "    def get_minibatch(self):\n",
    "        \"\"\"\n",
    "        Returns a minibatch of self.batch_size = 32 transitions\n",
    "        \"\"\"\n",
    "        if self.count < self.agent_history_length:\n",
    "            raise ValueError('Not enough memories to get a minibatch')\n",
    "        \n",
    "        self._get_valid_indices()\n",
    "            \n",
    "        for i, idx in enumerate(self.indices):\n",
    "            self.states[i] = self._get_state(idx - 1)\n",
    "            self.new_states[i] = self._get_state(idx)\n",
    "        \n",
    "        return np.transpose(self.states, axes=(0, 2, 3, 1)), self.actions[self.indices], self.rewards[self.indices], np.transpose(self.new_states, axes=(0, 2, 3, 1)), self.terminal_flags[self.indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(session, replay_memory, main_dqn, target_dqn, batch_size, gamma):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        session: A tensorflow sesson object\n",
    "        replay_memory: A ReplayMemory object\n",
    "        main_dqn: A DQN object\n",
    "        target_dqn: A DQN object\n",
    "        batch_size: Integer, Batch size\n",
    "        gamma: Float, discount factor for the Bellman equation\n",
    "    Returns:\n",
    "        loss: The loss of the minibatch, for tensorboard\n",
    "    Draws a minibatch from the replay memory, calculates the \n",
    "    target Q-value that the prediction Q-value is regressed to. \n",
    "    Then a parameter update is performed on the main DQN.\n",
    "    \"\"\"\n",
    "    # Draw a minibatch from the replay memory\n",
    "    states, actions, rewards, new_states, terminal_flags = replay_memory.get_minibatch()    \n",
    "    # The main network estimates which action is best (in the next \n",
    "    # state s', new_states is passed!) \n",
    "    # for every transition in the minibatch\n",
    "    arg_q_max = session.run(main_dqn.best_action, feed_dict={main_dqn.input:new_states})\n",
    "    # The target network estimates the Q-values (in the next state s', new_states is passed!) \n",
    "    # for every transition in the minibatch\n",
    "    q_vals = session.run(target_dqn.q_values, feed_dict={target_dqn.input:new_states})\n",
    "    double_q = q_vals[range(batch_size), arg_q_max]\n",
    "    # Bellman equation. Multiplication with (1-terminal_flags) makes sure that \n",
    "    # if the game is over, targetQ=rewards\n",
    "    target_q = rewards + (gamma*double_q * (1-terminal_flags))\n",
    "    # Gradient descend step to update the parameters of the main network\n",
    "    loss, _ = session.run([main_dqn.loss, main_dqn.update], \n",
    "                          feed_dict={main_dqn.input:states, \n",
    "                                     main_dqn.target_q:target_q, \n",
    "                                     main_dqn.action:actions})\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TargetNetworkUpdater:\n",
    "    \"\"\"Copies the parameters of the main DQN to the target DQN\"\"\"\n",
    "    def __init__(self, main_dqn_vars, target_dqn_vars):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            main_dqn_vars: A list of tensorflow variables belonging to the main DQN network\n",
    "            target_dqn_vars: A list of tensorflow variables belonging to the target DQN network\n",
    "        \"\"\"\n",
    "        self.main_dqn_vars = main_dqn_vars\n",
    "        self.target_dqn_vars = target_dqn_vars\n",
    "\n",
    "    def _update_target_vars(self):\n",
    "        update_ops = []\n",
    "        for i, var in enumerate(self.main_dqn_vars):\n",
    "            copy_op = self.target_dqn_vars[i].assign(var.value())\n",
    "            update_ops.append(copy_op)\n",
    "        return update_ops\n",
    "            \n",
    "    def update_networks(self, sess):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sess: A Tensorflow session object\n",
    "        Assigns the values of the parameters of the main network to the \n",
    "        parameters of the target network\n",
    "        \"\"\"\n",
    "        update_ops = self._update_target_vars()\n",
    "        for copy_op in update_ops:\n",
    "            sess.run(copy_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Atari:\n",
    "    \"\"\"Wrapper for the environment provided by gym\"\"\"\n",
    "    def __init__(self, envName, no_op_steps=10, agent_history_length=4):\n",
    "        self.env = gym.make(envName)\n",
    "        self.frame_processor = FrameProcessor()\n",
    "        self.state = None\n",
    "        self.last_lives = 0\n",
    "        self.no_op_steps = no_op_steps\n",
    "        self.agent_history_length = agent_history_length\n",
    "\n",
    "    def reset(self, sess, evaluation=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sess: A Tensorflow session object\n",
    "            evaluation: A boolean saying whether the agent is evaluating or training\n",
    "        Resets the environment and stacks four frames ontop of each other to \n",
    "        create the first state\n",
    "        \"\"\"\n",
    "        frame = self.env.reset()\n",
    "        self.last_lives = 0\n",
    "        terminal_life_lost = True # Set to true so that the agent starts \n",
    "                                  # with a 'FIRE' action when evaluating\n",
    "        if evaluation:\n",
    "            for _ in range(random.randint(1, self.no_op_steps)):\n",
    "                frame, _, _, _ = self.env.step(1) # Action 'Fire'\n",
    "        processed_frame = self.frame_processor.process(sess, frame)   # (★★★)\n",
    "        self.state = np.repeat(processed_frame, self.agent_history_length, axis=2)\n",
    "        \n",
    "        return terminal_life_lost\n",
    "\n",
    "    def step(self, sess, action):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sess: A Tensorflow session object\n",
    "            action: Integer, action the agent performs\n",
    "        Performs an action and observes the reward and terminal state from the environment\n",
    "        \"\"\"\n",
    "        new_frame, reward, terminal, info = self.env.step(action)  # (5★)\n",
    "            \n",
    "        if info['ale.lives'] < self.last_lives:\n",
    "            terminal_life_lost = True\n",
    "        else:\n",
    "            terminal_life_lost = terminal\n",
    "        self.last_lives = info['ale.lives']\n",
    "        \n",
    "        processed_new_frame = self.frame_processor.process(sess, new_frame)   # (6★)\n",
    "        new_state = np.append(self.state[:, :, 1:], processed_new_frame, axis=2) # (6★)   \n",
    "        self.state = new_state\n",
    "        \n",
    "        return processed_new_frame, reward, terminal, terminal_life_lost, new_frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The environment has the following 4 actions: ['NOOP', 'FIRE', 'RIGHT', 'LEFT']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tomault/src/thirdparty/openai-gym/gym/__init__.py:22: UserWarning: DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.\n",
      "  warnings.warn('DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.')\n"
     ]
    }
   ],
   "source": [
    "ENV_NAME = 'BreakoutDeterministic-v4'\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Control parameters\n",
    "MAX_EPISODE_LENGTH = 18000       # Equivalent of 5 minutes of gameplay at 60 frames per second\n",
    "EVAL_FREQUENCY = 200000          # Number of frames the agent sees between evaluations\n",
    "EVAL_STEPS = 10000               # Number of frames for one evaluation\n",
    "NETW_UPDATE_FREQ = 10000         # Number of chosen actions between updating the target network. \n",
    "                                 # According to Mnih et al. 2015 this is measured in the number of \n",
    "                                 # parameter updates (every four actions), however, in the \n",
    "                                 # DeepMind code, it is clearly measured in the number\n",
    "                                 # of actions the agent choses\n",
    "DISCOUNT_FACTOR = 0.99           # gamma in the Bellman equation\n",
    "REPLAY_MEMORY_START_SIZE = 50000 # Number of completely random actions, \n",
    "                                 # before the agent starts learning\n",
    "MAX_FRAMES = 30000000            # Total number of frames the agent sees \n",
    "MEMORY_SIZE = 1000000            # Number of transitions stored in the replay memory\n",
    "NO_OP_STEPS = 10                 # Number of 'NOOP' or 'FIRE' actions at the beginning of an \n",
    "                                 # evaluation episode\n",
    "UPDATE_FREQ = 4                  # Every four actions a gradient descend step is performed\n",
    "HIDDEN = 1024                    # Number of filters in the final convolutional layer. The output \n",
    "                                 # has the shape (1,1,1024) which is split into two streams. Both \n",
    "                                 # the advantage stream and value stream have the shape \n",
    "                                 # (1,1,512). This is slightly different from the original \n",
    "                                 # implementation but tests I did with the environment Pong \n",
    "                                 # have shown that this way the score increases more quickly\n",
    "LEARNING_RATE = 0.00001          # Set to 0.00025 in Pong for quicker results\n",
    "BS = 32                          # Batch size\n",
    "\n",
    "PATH = \"output/\"                 # Gifs and checkpoints will be saved here\n",
    "SUMMARIES = \"summaries\"          # logdir for tensorboard\n",
    "RUNID = 'run_1'\n",
    "#os.makedirs(PATH, exist_ok=True)\n",
    "#os.makedirs(os.path.join(SUMMARIES, RUNID))\n",
    "SUMM_WRITER = tf.summary.FileWriter(os.path.join(SUMMARIES, RUNID))\n",
    "\n",
    "atari = Atari(ENV_NAME, NO_OP_STEPS)\n",
    "\n",
    "print(\"The environment has the following {} actions: {}\".format(atari.env.action_space.n, \n",
    "                                                                atari.env.unwrapped.get_action_meanings()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main DQN and target DQN networks:\n",
    "with tf.device(\"/gpu:0\"):\n",
    "    with tf.variable_scope('mainDQN'):\n",
    "        MAIN_DQN = DQN(atari.env.action_space.n, HIDDEN, LEARNING_RATE)   # (★★)\n",
    "    with tf.variable_scope('targetDQN'):\n",
    "        TARGET_DQN = DQN(atari.env.action_space.n, HIDDEN)               # (★★)\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    MAIN_DQN_VARS = tf.trainable_variables(scope='mainDQN')\n",
    "    TARGET_DQN_VARS = tf.trainable_variables(scope='targetDQN')\n",
    "    \n",
    "saver = tf.train.Saver()    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAYER_IDS = [\"conv1\", \"conv2\", \"conv3\", \"conv4\", \"denseAdvantage\", \n",
    "             \"denseAdvantageBias\", \"denseValue\", \"denseValueBias\"]\n",
    "\n",
    "# Scalar summariess for tensorboard: loss, average reward and evaluation score\n",
    "with tf.name_scope('Performance'):\n",
    "    LOSS_PH = tf.placeholder(tf.float32, shape=None, name='loss_summary')\n",
    "    LOSS_SUMMARY = tf.summary.scalar('loss', LOSS_PH)\n",
    "    REWARD_PH = tf.placeholder(tf.float32, shape=None, name='reward_summary')\n",
    "    REWARD_SUMMARY = tf.summary.scalar('reward', REWARD_PH)\n",
    "    EVAL_SCORE_PH = tf.placeholder(tf.float32, shape=None, name='evaluation_summary')\n",
    "    EVAL_SCORE_SUMMARY = tf.summary.scalar('evaluation_score', EVAL_SCORE_PH)\n",
    "\n",
    "PERFORMANCE_SUMMARIES = tf.summary.merge([LOSS_SUMMARY, REWARD_SUMMARY])\n",
    "\n",
    "with tf.device(\"/gpu:0\"):\n",
    "    # Histogramm summaries for tensorboard: parameters\n",
    "    with tf.name_scope('Parameters'):\n",
    "        ALL_PARAM_SUMMARIES = []\n",
    "        for i, Id in enumerate(LAYER_IDS):\n",
    "            with tf.name_scope('mainDQN/'):\n",
    "                MAIN_DQN_KERNEL = tf.summary.histogram(Id, tf.reshape(MAIN_DQN_VARS[i], shape=[-1]))\n",
    "            ALL_PARAM_SUMMARIES.extend([MAIN_DQN_KERNEL])\n",
    "    PARAM_SUMMARIES = tf.summary.merge(ALL_PARAM_SUMMARIES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    \"\"\"Contains the training and evaluation loops\"\"\"\n",
    "    with tf.device(\"/gpu:0\"):\n",
    "        my_replay_memory = ReplayMemory(size=MEMORY_SIZE, batch_size=BS)   # (★)\n",
    "        network_updater = TargetNetworkUpdater(MAIN_DQN_VARS, TARGET_DQN_VARS)\n",
    "        action_getter = ActionGetter(atari.env.action_space.n, \n",
    "                                     replay_memory_start_size=REPLAY_MEMORY_START_SIZE, \n",
    "                                     max_frames=MAX_FRAMES)\n",
    "        \n",
    "    config = tf.ConfigProto(allow_soft_placement = True, log_device_placement = True)\n",
    "    with tf.Session(config = config) as sess:\n",
    "        sess.run(init)\n",
    "        \n",
    "        frame_number = 0\n",
    "        rewards = []\n",
    "        loss_list = []\n",
    "        \n",
    "        while frame_number < MAX_FRAMES:\n",
    "            \n",
    "            ########################\n",
    "            ####### Training #######\n",
    "            ########################\n",
    "            epoch_frame = 0\n",
    "            while epoch_frame < EVAL_FREQUENCY:\n",
    "                terminal_life_lost = atari.reset(sess)\n",
    "                episode_reward_sum = 0\n",
    "                for _ in range(MAX_EPISODE_LENGTH):\n",
    "                    # (4★)\n",
    "                    action = action_getter.get_action(sess, frame_number, atari.state, MAIN_DQN)   \n",
    "                    # (5★)\n",
    "                    processed_new_frame, reward, terminal, terminal_life_lost, _ = atari.step(sess, action)  \n",
    "                    frame_number += 1\n",
    "                    epoch_frame += 1\n",
    "                    episode_reward_sum += reward\n",
    "                    \n",
    "                    # (7★) Store transition in the replay memory\n",
    "                    my_replay_memory.add_experience(action=action, \n",
    "                                                    frame=processed_new_frame[:, :, 0],\n",
    "                                                    reward=reward, \n",
    "                                                    terminal=terminal_life_lost)   \n",
    "                    \n",
    "                    if frame_number % UPDATE_FREQ == 0 and frame_number > REPLAY_MEMORY_START_SIZE:\n",
    "                        loss = learn(sess, my_replay_memory, MAIN_DQN, TARGET_DQN,\n",
    "                                     BS, gamma = DISCOUNT_FACTOR) # (8★)\n",
    "                        loss_list.append(loss)\n",
    "                    if frame_number % NETW_UPDATE_FREQ == 0 and frame_number > REPLAY_MEMORY_START_SIZE:\n",
    "                        network_updater.update_networks(sess) # (9★)\n",
    "                    \n",
    "                    if terminal:\n",
    "                        terminal = False\n",
    "                        break\n",
    "\n",
    "                rewards.append(episode_reward_sum)\n",
    "                \n",
    "                # Output the progress:\n",
    "                if len(rewards) % 10 == 0:\n",
    "                    # Scalar summaries for tensorboard\n",
    "                    if frame_number > REPLAY_MEMORY_START_SIZE:\n",
    "                        summ = sess.run(PERFORMANCE_SUMMARIES, \n",
    "                                        feed_dict={LOSS_PH:np.mean(loss_list), \n",
    "                                                   REWARD_PH:np.mean(rewards[-100:])})\n",
    "                        \n",
    "                        SUMM_WRITER.add_summary(summ, frame_number)\n",
    "                        loss_list = []\n",
    "                    # Histogramm summaries for tensorboard\n",
    "                    summ_param = sess.run(PARAM_SUMMARIES)\n",
    "                    SUMM_WRITER.add_summary(summ_param, frame_number)\n",
    "                    \n",
    "                    dt_now = datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S')\n",
    "                    print(dt_now, len(rewards), frame_number, np.mean(rewards[-100:]))\n",
    "                    with open('rewards.dat', 'a') as reward_file:\n",
    "                        reward_file.write('%s: %d %s %s\\n' % (dt_now, len(rewards), frame_number, np.mean(rewards[-100:])))\n",
    "            \n",
    "            ########################\n",
    "            ###### Evaluation ######\n",
    "            ########################\n",
    "            terminal = True\n",
    "            gif = False\n",
    "            frames_for_gif = []\n",
    "            eval_rewards = []\n",
    "            evaluate_frame_number = 0\n",
    "\n",
    "            for _ in range(EVAL_STEPS):\n",
    "                if terminal:\n",
    "                    terminal_life_lost = atari.reset(sess, evaluation=True)\n",
    "                    episode_reward_sum = 0\n",
    "                    terminal = False\n",
    "               \n",
    "                # Fire (action 1), when a life was lost or the game just started, \n",
    "                # so that the agent does not stand around doing nothing. When playing \n",
    "                # with other environments, you might want to change this...\n",
    "                action = 1 if terminal_life_lost else action_getter.get_action(sess, frame_number,\n",
    "                                                                               atari.state, \n",
    "                                                                               MAIN_DQN,\n",
    "                                                                               evaluation=True)\n",
    "                processed_new_frame, reward, terminal, terminal_life_lost, new_frame = atari.step(sess, action)\n",
    "                evaluate_frame_number += 1\n",
    "                episode_reward_sum += reward\n",
    "\n",
    "                if gif: \n",
    "                    frames_for_gif.append(new_frame)\n",
    "                if terminal:\n",
    "                    eval_rewards.append(episode_reward_sum)\n",
    "                    gif = False # Save only the first game of the evaluation as a gif\n",
    "                     \n",
    "            print(\"Evaluation score:\\n\", np.mean(eval_rewards))       \n",
    "#            try:\n",
    "#                generate_gif(frame_number, frames_for_gif, eval_rewards[0], PATH)\n",
    "#            except IndexError:\n",
    "#                print(\"No evaluation game finished\")\n",
    "            \n",
    "            #Save the network parameters\n",
    "            saver.save(sess, PATH+'/my_model', global_step=frame_number)\n",
    "            frames_for_gif = []\n",
    "            \n",
    "            # Show the evaluation score in tensorboard\n",
    "            summ = sess.run(EVAL_SCORE_SUMMARY, feed_dict={EVAL_SCORE_PH:np.mean(eval_rewards)})\n",
    "            SUMM_WRITER.add_summary(summ, frame_number)\n",
    "            with open('rewardsEval.dat', 'a') as eval_reward_file:\n",
    "                eval_reward_file.write('%s %s\\n' % (frame_number, np.mean(eval_rewards)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('2018-09-02T08:25:15', 10, 1725, 1.1)\n",
      "('2018-09-02T08:25:24', 20, 3679, 1.35)\n",
      "('2018-09-02T08:25:33', 30, 5409, 1.2)\n",
      "('2018-09-02T08:25:42', 40, 7315, 1.25)\n",
      "('2018-09-02T08:25:50', 50, 9081, 1.24)\n",
      "('2018-09-02T08:25:58', 60, 10695, 1.15)\n",
      "('2018-09-02T08:26:08', 70, 12344, 1.1)\n",
      "('2018-09-02T08:26:15', 80, 13911, 1.025)\n",
      "('2018-09-02T08:26:26', 90, 15908, 1.0888888888888888)\n",
      "('2018-09-02T08:26:34', 100, 17538, 1.03)\n",
      "('2018-09-02T08:26:44', 110, 19194, 0.99)\n",
      "('2018-09-02T08:26:56', 120, 21276, 1.01)\n",
      "('2018-09-02T08:27:05', 130, 23059, 1.04)\n",
      "('2018-09-02T08:27:14', 140, 25076, 1.08)\n",
      "('2018-09-02T08:27:22', 150, 26957, 1.1)\n",
      "('2018-09-02T08:27:31', 160, 28615, 1.1)\n",
      "('2018-09-02T08:27:39', 170, 30298, 1.1)\n",
      "('2018-09-02T08:27:48', 180, 32358, 1.23)\n",
      "('2018-09-02T08:27:58', 190, 34071, 1.18)\n",
      "('2018-09-02T08:28:08', 200, 35782, 1.23)\n",
      "('2018-09-02T08:28:16', 210, 37470, 1.25)\n",
      "('2018-09-02T08:28:25', 220, 39558, 1.25)\n",
      "('2018-09-02T08:28:35', 230, 41658, 1.31)\n",
      "('2018-09-02T08:28:43', 240, 43423, 1.25)\n",
      "('2018-09-02T08:28:51', 250, 45203, 1.22)\n",
      "('2018-09-02T08:28:59', 260, 47025, 1.28)\n",
      "('2018-09-02T08:29:10', 270, 48829, 1.33)\n",
      "('2018-09-02T08:29:23', 280, 50410, 1.22)\n",
      "('2018-09-02T08:29:49', 290, 52295, 1.24)\n",
      "('2018-09-02T08:30:16', 300, 54137, 1.27)\n",
      "('2018-09-02T08:30:40', 310, 55839, 1.27)\n",
      "('2018-09-02T08:31:06', 320, 57605, 1.21)\n",
      "('2018-09-02T08:31:30', 330, 59315, 1.11)\n",
      "('2018-09-02T08:31:55', 340, 60950, 1.07)\n",
      "('2018-09-02T08:32:19', 350, 62491, 1.01)\n",
      "('2018-09-02T08:32:44', 360, 64283, 0.99)\n",
      "('2018-09-02T08:33:12', 370, 66194, 0.99)\n",
      "('2018-09-02T08:33:37', 380, 67954, 1.02)\n",
      "('2018-09-02T08:34:01', 390, 69697, 0.98)\n",
      "('2018-09-02T08:34:28', 400, 71586, 0.99)\n",
      "('2018-09-02T08:34:50', 410, 73134, 0.95)\n",
      "('2018-09-02T08:35:14', 420, 74837, 0.92)\n",
      "('2018-09-02T08:35:39', 430, 76638, 0.96)\n",
      "('2018-09-02T08:36:01', 440, 78262, 0.94)\n",
      "('2018-09-02T08:36:27', 450, 80030, 0.99)\n",
      "('2018-09-02T08:36:56', 460, 82019, 1.05)\n",
      "('2018-09-02T08:37:20', 470, 83707, 1.02)\n",
      "('2018-09-02T08:37:44', 480, 85423, 1.02)\n",
      "('2018-09-02T08:38:11', 490, 87256, 1.04)\n",
      "('2018-09-02T08:38:37', 500, 89048, 1.01)\n",
      "('2018-09-02T08:39:01', 510, 90715, 1.04)\n",
      "('2018-09-02T08:39:26', 520, 92493, 1.07)\n",
      "('2018-09-02T08:39:51', 530, 94300, 1.09)\n",
      "('2018-09-02T08:40:17', 540, 96089, 1.13)\n",
      "('2018-09-02T08:40:42', 550, 97889, 1.17)\n",
      "('2018-09-02T08:41:09', 560, 99773, 1.15)\n",
      "('2018-09-02T08:41:32', 570, 101373, 1.12)\n",
      "('2018-09-02T08:41:58', 580, 103272, 1.15)\n",
      "('2018-09-02T08:42:22', 590, 105030, 1.16)\n",
      "('2018-09-02T08:42:49', 600, 106811, 1.16)\n",
      "('2018-09-02T08:43:13', 610, 108449, 1.16)\n",
      "('2018-09-02T08:43:42', 620, 110459, 1.21)\n",
      "('2018-09-02T08:44:10', 630, 112356, 1.21)\n",
      "('2018-09-02T08:44:32', 640, 113868, 1.16)\n",
      "('2018-09-02T08:44:56', 650, 115576, 1.12)\n",
      "('2018-09-02T08:45:23', 660, 117500, 1.13)\n",
      "('2018-09-02T08:45:50', 670, 119404, 1.2)\n",
      "('2018-09-02T08:46:13', 680, 121047, 1.15)\n",
      "('2018-09-02T08:46:37', 690, 122756, 1.13)\n",
      "('2018-09-02T08:47:00', 700, 124328, 1.07)\n",
      "('2018-09-02T08:47:24', 710, 126024, 1.08)\n",
      "('2018-09-02T08:47:49', 720, 127757, 1.01)\n",
      "('2018-09-02T08:48:13', 730, 129519, 0.99)\n",
      "('2018-09-02T08:48:37', 740, 131205, 1.03)\n",
      "('2018-09-02T08:49:01', 750, 132857, 1.01)\n",
      "('2018-09-02T08:49:25', 760, 134500, 0.93)\n",
      "('2018-09-02T08:49:54', 770, 136416, 0.94)\n",
      "('2018-09-02T08:50:17', 780, 138078, 0.94)\n",
      "('2018-09-02T08:50:43', 790, 139922, 0.95)\n",
      "('2018-09-02T08:51:12', 800, 142018, 1.08)\n",
      "('2018-09-02T08:51:38', 810, 143873, 1.12)\n",
      "('2018-09-02T08:52:04', 820, 145645, 1.15)\n",
      "('2018-09-02T08:52:28', 830, 147355, 1.14)\n",
      "('2018-09-02T08:52:52', 840, 148959, 1.12)\n",
      "('2018-09-02T08:53:14', 850, 150539, 1.1)\n",
      "('2018-09-02T08:53:40', 860, 152372, 1.15)\n",
      "('2018-09-02T08:54:01', 870, 153865, 1.05)\n",
      "('2018-09-02T08:54:29', 880, 155844, 1.13)\n",
      "('2018-09-02T08:54:58', 890, 157865, 1.19)\n",
      "('2018-09-02T08:55:22', 900, 159535, 1.11)\n",
      "('2018-09-02T08:55:47', 910, 161310, 1.1)\n",
      "('2018-09-02T08:56:11', 920, 163061, 1.08)\n",
      "('2018-09-02T08:56:39', 930, 165149, 1.16)\n",
      "('2018-09-02T08:57:05', 940, 167027, 1.24)\n",
      "('2018-09-02T08:57:30', 950, 168820, 1.31)\n",
      "('2018-09-02T08:57:56', 960, 170653, 1.31)\n",
      "('2018-09-02T08:58:17', 970, 172119, 1.29)\n",
      "('2018-09-02T08:58:39', 980, 173683, 1.19)\n",
      "('2018-09-02T08:59:07', 990, 175649, 1.17)\n",
      "('2018-09-02T08:59:31', 1000, 177345, 1.18)\n",
      "('2018-09-02T08:59:54', 1010, 179047, 1.16)\n",
      "('2018-09-02T09:00:20', 1020, 180849, 1.17)\n",
      "('2018-09-02T09:00:47', 1030, 182690, 1.12)\n",
      "('2018-09-02T09:01:16', 1040, 184680, 1.16)\n",
      "('2018-09-02T09:01:39', 1050, 186320, 1.12)\n",
      "('2018-09-02T09:02:07', 1060, 188215, 1.12)\n",
      "('2018-09-02T09:02:38', 1070, 190407, 1.3)\n",
      "('2018-09-02T09:03:05', 1080, 192287, 1.4)\n",
      "('2018-09-02T09:03:33', 1090, 194277, 1.41)\n",
      "('2018-09-02T09:04:04', 1100, 196344, 1.5)\n",
      "('2018-09-02T09:04:28', 1110, 197989, 1.49)\n",
      "('2018-09-02T09:04:54', 1120, 199859, 1.5)\n",
      "('Evaluation score:\\n', 3.0)\n",
      "('2018-09-02T09:06:32', 1130, 201688, 1.5)\n",
      "('2018-09-02T09:07:00', 1140, 203619, 1.48)\n",
      "('2018-09-02T09:07:29', 1150, 205569, 1.54)\n",
      "('2018-09-02T09:07:56', 1160, 207497, 1.57)\n",
      "('2018-09-02T09:08:22', 1170, 209220, 1.47)\n",
      "('2018-09-02T09:08:51', 1180, 211258, 1.49)\n",
      "('2018-09-02T09:09:13', 1190, 212798, 1.4)\n",
      "('2018-09-02T09:09:36', 1200, 214421, 1.28)\n",
      "('2018-09-02T09:10:01', 1210, 216133, 1.28)\n",
      "('2018-09-02T09:10:29', 1220, 218110, 1.32)\n",
      "('2018-09-02T09:10:55', 1230, 219907, 1.32)\n",
      "('2018-09-02T09:11:26', 1240, 222044, 1.35)\n",
      "('2018-09-02T09:11:53', 1250, 223967, 1.34)\n",
      "('2018-09-02T09:12:24', 1260, 226153, 1.4)\n",
      "('2018-09-02T09:12:49', 1270, 227866, 1.4)\n",
      "('2018-09-02T09:13:15', 1280, 229709, 1.39)\n",
      "('2018-09-02T09:13:41', 1290, 231536, 1.44)\n",
      "('2018-09-02T09:14:06', 1300, 233205, 1.46)\n",
      "('2018-09-02T09:14:36', 1310, 235314, 1.55)\n",
      "('2018-09-02T09:15:06', 1320, 237385, 1.56)\n",
      "('2018-09-02T09:15:35', 1330, 239412, 1.61)\n",
      "('2018-09-02T09:16:06', 1340, 241486, 1.62)\n",
      "('2018-09-02T09:16:36', 1350, 243591, 1.71)\n",
      "('2018-09-02T09:17:05', 1360, 245601, 1.66)\n",
      "('2018-09-02T09:17:32', 1370, 247537, 1.72)\n",
      "('2018-09-02T09:18:06', 1380, 249788, 1.81)\n",
      "('2018-09-02T09:18:35', 1390, 251752, 1.87)\n",
      "('2018-09-02T09:19:09', 1400, 254074, 2.02)\n",
      "('2018-09-02T09:19:38', 1410, 256131, 2.04)\n",
      "('2018-09-02T09:20:04', 1420, 257952, 2.0)\n",
      "('2018-09-02T09:20:34', 1430, 259990, 1.99)\n",
      "('2018-09-02T09:21:03', 1440, 261976, 1.95)\n",
      "('2018-09-02T09:21:35', 1450, 264138, 1.96)\n",
      "('2018-09-02T09:22:02', 1460, 266045, 1.94)\n",
      "('2018-09-02T09:22:30', 1470, 267995, 1.95)\n",
      "('2018-09-02T09:23:02', 1480, 270150, 1.92)\n",
      "('2018-09-02T09:23:35', 1490, 272307, 1.96)\n",
      "('2018-09-02T09:24:12', 1500, 274774, 2.01)\n",
      "('2018-09-02T09:24:39', 1510, 276653, 1.97)\n",
      "('2018-09-02T09:25:09', 1520, 278722, 2.05)\n",
      "('2018-09-02T09:25:40', 1530, 280797, 2.07)\n",
      "('2018-09-02T09:26:13', 1540, 283131, 2.16)\n",
      "('2018-09-02T09:26:42', 1550, 285065, 2.08)\n",
      "('2018-09-02T09:27:18', 1560, 287577, 2.23)\n",
      "('2018-09-02T09:27:51', 1570, 289869, 2.29)\n",
      "('2018-09-02T09:28:27', 1580, 292134, 2.29)\n",
      "('2018-09-02T09:29:01', 1590, 294336, 2.33)\n",
      "('2018-09-02T09:29:40', 1600, 296948, 2.36)\n",
      "('2018-09-02T09:30:12', 1610, 299146, 2.45)\n",
      "('2018-09-02T09:30:48', 1620, 301504, 2.51)\n",
      "('2018-09-02T09:31:24', 1630, 303887, 2.61)\n",
      "('2018-09-02T09:32:00', 1640, 306358, 2.66)\n",
      "('2018-09-02T09:32:34', 1650, 308686, 2.79)\n",
      "('2018-09-02T09:33:08', 1660, 310893, 2.75)\n",
      "('2018-09-02T09:33:40', 1670, 312975, 2.73)\n",
      "('2018-09-02T09:34:19', 1680, 315476, 2.8)\n",
      "('2018-09-02T09:34:57', 1690, 318011, 2.85)\n",
      "('2018-09-02T09:35:30', 1700, 320333, 2.83)\n",
      "('2018-09-02T09:36:04', 1710, 322646, 2.86)\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
